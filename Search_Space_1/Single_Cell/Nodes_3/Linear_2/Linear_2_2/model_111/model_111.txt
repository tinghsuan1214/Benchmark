--------------------Training--------------------
arch_str :: |skip_connect~0|+|lstm_3~0|lstm_1~1|[linear->dropout->linear]
model :: 3F
InferCell(
  info :: nodes=3, inC=1, outC=64, [1<-(I0-L0) | 2<-(I0-L1,I1-L2)], genotype_str: skip_connect~0|lstm_3~0|lstm_1~1
  linear_layers: [linear->dropout->linear]
  (layers): ModuleList(
    (0): FactorizedReduce(
      C_in=1, C_out=64, stride=1
      (relu): ReLU()
      (conv): Conv1d(1, 64, kernel_size=(1,), stride=(1,), bias=False)
      (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): LSTM(
      (lstm): LSTM(1, 64, num_layers=3, batch_first=True)
    )
    (2): LSTM(
      (lstm): LSTM(64, 64, batch_first=True)
    )
  )
  (linear_layers): ModuleList(
    (0): Linear(in_features=3072, out_features=1536, bias=True)
    (1): Dropout(p=0.1, inplace=False)
    (2): Linear(in_features=1536, out_features=1, bias=True)
  )
)
tr_params :: epochs = 100, patience = 20, batch_size = 256, window_size = 48, data_size = 100
Model MACs: 10.449M, Model Params: 4.839M
learning rate scheduling :: (optimizer, mode='min', factor=0.1, patience=5, verbose=True)
total_samples :: train_dataset = 2136000, train_dataloader = 2136000
total_samples :: valid_dataset = 283200, valid_dataloader = 283200
Epoch ::  1 || Loss: 0.43110756 || it_count: 8344 || Val Loss: 0.45914717 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 00:06:50.21
Epoch ::  2 || Loss: 0.41495858 || it_count: 8344 || Val Loss: 0.45375238 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 00:13:32.94
Epoch ::  3 || Loss: 0.41421790 || it_count: 8344 || Val Loss: 0.44984187 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 00:20:15.37
Epoch ::  4 || Loss: 0.41288681 || it_count: 8344 || Val Loss: 0.45210537 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 00:26:58.07
Epoch ::  5 || Loss: 0.41217080 || it_count: 8344 || Val Loss: 0.45158690 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 00:33:41.32
Epoch ::  6 || Loss: 0.41158309 || it_count: 8344 || Val Loss: 0.45267935 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 00:40:33.62
Epoch ::  7 || Loss: 0.41147050 || it_count: 8344 || Val Loss: 0.45420682 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 00:47:31.68
Epoch ::  8 || Loss: 0.41121847 || it_count: 8344 || Val Loss: 0.45176358 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 00:54:16.06
Epoch ::  9 || Loss: 0.41113994 || it_count: 8344 || Val Loss: 0.45274516 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 01:01:0.98
Epoch :: 10 || Loss: 0.41077586 || it_count: 8344 || Val Loss: 0.45431441 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 01:07:48.49
Epoch :: 11 || Loss: 0.41056707 || it_count: 8344 || Val Loss: 0.45194700 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 01:14:36.39
Epoch :: 12 || Loss: 0.41004948 || it_count: 8344 || Val Loss: 0.45340102 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 01:21:24.85
Epoch :: 13 || Loss: 0.40998991 || it_count: 8344 || Val Loss: 0.45300721 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 01:28:11.73
Epoch :: 14 || Loss: 0.40993665 || it_count: 8344 || Val Loss: 0.45419713 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 01:35:0.67
Epoch :: 15 || Loss: 0.40924052 || it_count: 8344 || Val Loss: 0.45417037 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 01:41:48.70
Epoch :: 16 || Loss: 0.40872379 || it_count: 8344 || Val Loss: 0.45506915 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 01:48:43.73
Epoch :: 17 || Loss: 0.40866346 || it_count: 8344 || Val Loss: 0.45484351 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 01:55:46.48
Epoch :: 18 || Loss: 0.40840387 || it_count: 8344 || Val Loss: 0.45460252 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 02:02:35.75
Epoch :: 19 || Loss: 0.40816824 || it_count: 8344 || Val Loss: 0.45320177 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 02:09:22.18
Epoch :: 20 || Loss: 0.40773981 || it_count: 8344 || Val Loss: 0.45344738 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 02:16:9.64
Epoch :: 21 || Loss: 0.40745587 || it_count: 8344 || Val Loss: 0.45424985 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 02:22:55.89
Epoch :: 22 || Loss: 0.40717040 || it_count: 8344 || Val Loss: 0.44822139 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 02:29:42.14
Epoch :: 23 || Loss: 0.40658357 || it_count: 8344 || Val Loss: 0.44932854 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 02:36:28.62
Epoch :: 24 || Loss: 0.40637986 || it_count: 8344 || Val Loss: 0.45135546 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 02:43:15.12
Epoch :: 25 || Loss: 0.40586700 || it_count: 8344 || Val Loss: 0.45107321 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 02:50:2.27
Epoch :: 26 || Loss: 0.40558618 || it_count: 8344 || Val Loss: 0.45235645 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 02:56:57.18
Epoch :: 27 || Loss: 0.40525335 || it_count: 8344 || Val Loss: 0.45363809 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 03:03:58.37
Epoch :: 28 || Loss: 0.40482193 || it_count: 8344 || Val Loss: 0.45275233 || Val it_count: 1107 || Current Learning Rate: 0.0001 || Time: 03:10:50.24
Epoch :: 29 || Loss: 0.41128147 || it_count: 8344 || Val Loss: 0.44789869 || Val it_count: 1107 || Current Learning Rate: 0.0001 || Time: 03:17:36.77
Epoch :: 30 || Loss: 0.40819241 || it_count: 8344 || Val Loss: 0.44828964 || Val it_count: 1107 || Current Learning Rate: 0.0001 || Time: 03:24:23.55
Epoch :: 31 || Loss: 0.40713194 || it_count: 8344 || Val Loss: 0.44859931 || Val it_count: 1107 || Current Learning Rate: 0.0001 || Time: 03:31:10.42
Epoch :: 32 || Loss: 0.40638511 || it_count: 8344 || Val Loss: 0.44857850 || Val it_count: 1107 || Current Learning Rate: 0.0001 || Time: 03:37:58.01
Epoch :: 33 || Loss: 0.40578775 || it_count: 8344 || Val Loss: 0.44850041 || Val it_count: 1107 || Current Learning Rate: 0.0001 || Time: 03:44:45.74
Epoch :: 34 || Loss: 0.40530020 || it_count: 8344 || Val Loss: 0.44815524 || Val it_count: 1107 || Current Learning Rate: 0.0001 || Time: 03:51:33.81
Epoch :: 35 || Loss: 0.40490975 || it_count: 8344 || Val Loss: 0.44783428 || Val it_count: 1107 || Current Learning Rate: 0.0001 || Time: 03:58:21.38
Epoch :: 36 || Loss: 0.40460065 || it_count: 8344 || Val Loss: 0.44803469 || Val it_count: 1107 || Current Learning Rate: 0.0001 || Time: 04:05:13.91
Epoch :: 37 || Loss: 0.40438297 || it_count: 8344 || Val Loss: 0.44772673 || Val it_count: 1107 || Current Learning Rate: 0.0001 || Time: 04:12:13.11
Epoch :: 38 || Loss: 0.40408696 || it_count: 8344 || Val Loss: 0.44774664 || Val it_count: 1107 || Current Learning Rate: 0.0001 || Time: 04:19:10.09
Epoch :: 39 || Loss: 0.40385498 || it_count: 8344 || Val Loss: 0.44738430 || Val it_count: 1107 || Current Learning Rate: 0.0001 || Time: 04:25:57.41
Epoch :: 40 || Loss: 0.40356783 || it_count: 8344 || Val Loss: 0.44747701 || Val it_count: 1107 || Current Learning Rate: 0.0001 || Time: 04:32:45.41
Epoch :: 41 || Loss: 0.40334318 || it_count: 8344 || Val Loss: 0.44750451 || Val it_count: 1107 || Current Learning Rate: 0.0001 || Time: 04:39:32.46
Epoch :: 42 || Loss: 0.40314652 || it_count: 8344 || Val Loss: 0.44748950 || Val it_count: 1107 || Current Learning Rate: 0.0001 || Time: 04:46:19.16
Epoch :: 43 || Loss: 0.40294482 || it_count: 8344 || Val Loss: 0.44744691 || Val it_count: 1107 || Current Learning Rate: 0.0001 || Time: 04:52:52.14
Epoch :: 44 || Loss: 0.40272322 || it_count: 8344 || Val Loss: 0.44804123 || Val it_count: 1107 || Current Learning Rate: 0.0001 || Time: 04:59:40.80
Epoch :: 45 || Loss: 0.40246771 || it_count: 8344 || Val Loss: 0.44795582 || Val it_count: 1107 || Current Learning Rate: 1e-05 || Time: 05:06:29.58
Epoch :: 46 || Loss: 0.40542792 || it_count: 8344 || Val Loss: 0.44290144 || Val it_count: 1107 || Current Learning Rate: 1e-05 || Time: 05:13:20.53
Epoch :: 47 || Loss: 0.40379323 || it_count: 8344 || Val Loss: 0.44361157 || Val it_count: 1107 || Current Learning Rate: 1e-05 || Time: 05:20:13.96
Epoch :: 48 || Loss: 0.40356309 || it_count: 8344 || Val Loss: 0.44420577 || Val it_count: 1107 || Current Learning Rate: 1e-05 || Time: 05:27:6.67
Epoch :: 49 || Loss: 0.40341843 || it_count: 8344 || Val Loss: 0.44478179 || Val it_count: 1107 || Current Learning Rate: 1e-05 || Time: 05:34:0.42
Epoch :: 50 || Loss: 0.40329978 || it_count: 8344 || Val Loss: 0.44512266 || Val it_count: 1107 || Current Learning Rate: 1e-05 || Time: 05:40:56.63
Epoch :: 51 || Loss: 0.40319136 || it_count: 8344 || Val Loss: 0.44551519 || Val it_count: 1107 || Current Learning Rate: 1e-05 || Time: 05:47:56.76
Epoch :: 52 || Loss: 0.40309379 || it_count: 8344 || Val Loss: 0.44573653 || Val it_count: 1107 || Current Learning Rate: 1.0000000000000002e-06 || Time: 05:55:0.79
Epoch :: 53 || Loss: 0.40346966 || it_count: 8344 || Val Loss: 0.44387345 || Val it_count: 1107 || Current Learning Rate: 1.0000000000000002e-06 || Time: 06:02:5.72
Epoch :: 54 || Loss: 0.40325626 || it_count: 8344 || Val Loss: 0.44336755 || Val it_count: 1107 || Current Learning Rate: 1.0000000000000002e-06 || Time: 06:09:6.63
Epoch :: 55 || Loss: 0.40319720 || it_count: 8344 || Val Loss: 0.44314920 || Val it_count: 1107 || Current Learning Rate: 1.0000000000000002e-06 || Time: 06:16:4.25
Epoch :: 56 || Loss: 0.40317032 || it_count: 8344 || Val Loss: 0.44305052 || Val it_count: 1107 || Current Learning Rate: 1.0000000000000002e-06 || Time: 06:22:55.46
Epoch :: 57 || Loss: 0.40317019 || it_count: 8344 || Val Loss: 0.44303971 || Val it_count: 1107 || Current Learning Rate: 1.0000000000000002e-06 || Time: 06:29:47.23
Early stopping triggered due to learning rate below threshold.
Done Total time: 06:36:38.86
best_loss: 0.4429014361943056

--------------------Testing--------------------

total_samples :: test_dataset = 139200, test_dataloader = 139200
Epoch ::  1 || Loss: 0.31917786 || it_count: 544 || Time: 00:00:18.10
MAE:  0.28740197
MSE:  0.31923875
RMSE:  0.48184943
