--------------------Training--------------------
arch_str :: |lstm_1~0|+|skip_connect~0|lstm_3~1|[relu->linear->linear]
model :: 3M
InferCell(
  info :: nodes=3, inC=1, outC=64, [1<-(I0-L0) | 2<-(I0-L1,I1-L2)], genotype_str: lstm_1~0|skip_connect~0|lstm_3~1
  linear_layers: [relu->linear->linear]
  (layers): ModuleList(
    (0): LSTM(
      (lstm): LSTM(1, 64, batch_first=True)
    )
    (1): FactorizedReduce(
      C_in=1, C_out=64, stride=1
      (relu): ReLU()
      (conv): Conv1d(1, 64, kernel_size=(1,), stride=(1,), bias=False)
      (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): LSTM(
      (lstm): LSTM(64, 64, num_layers=3, batch_first=True)
    )
  )
  (linear_layers): ModuleList(
    (0): ReLU()
    (1): Linear(in_features=3072, out_features=1536, bias=True)
    (2): Linear(in_features=1536, out_features=1, bias=True)
  )
)
tr_params :: epochs = 100, patience = 20, batch_size = 256, window_size = 48, data_size = 100
Model FLOPs: 10.449M, Model Params: 4.839M
learning rate scheduling :: (optimizer, mode='min', factor=0.1, patience=5, verbose=True)
total_samples :: train_dataset = 2136000, train_dataloader = 2136000
total_samples :: valid_dataset = 283200, valid_dataloader = 283200
Epoch ::  1 || Loss: 0.46842006 || it_count: 8344 || Val Loss: 0.48150244 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 00:09:35.50
Epoch ::  2 || Loss: 0.44881890 || it_count: 8344 || Val Loss: 0.46561799 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 00:19:7.82
Epoch ::  3 || Loss: 0.45488257 || it_count: 8344 || Val Loss: 0.47323446 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 00:28:42.07
Epoch ::  4 || Loss: 0.44195141 || it_count: 8344 || Val Loss: 0.46284700 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 00:38:11.61
Epoch ::  5 || Loss: 0.43059300 || it_count: 8344 || Val Loss: 0.46893432 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 00:47:35.59
Epoch ::  6 || Loss: 0.42245195 || it_count: 8344 || Val Loss: 0.47671221 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 00:56:58.71
Epoch ::  7 || Loss: 0.41690885 || it_count: 8344 || Val Loss: 0.48683696 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 01:06:24.83
Epoch ::  8 || Loss: 0.41557689 || it_count: 8344 || Val Loss: 0.47810442 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 01:15:55.15
Epoch ::  9 || Loss: 0.41407972 || it_count: 8344 || Val Loss: 0.47426011 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 01:25:24.05
Epoch :: 10 || Loss: 0.41393219 || it_count: 8344 || Val Loss: 0.47017310 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 01:34:47.44
Epoch :: 11 || Loss: 0.41360328 || it_count: 8344 || Val Loss: 0.46373049 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 01:44:14.71
Epoch :: 12 || Loss: 0.41323987 || it_count: 8344 || Val Loss: 0.46614809 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 01:53:39.53
Epoch :: 13 || Loss: 0.41267818 || it_count: 8344 || Val Loss: 0.46291638 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 02:03:5.53
Epoch :: 14 || Loss: 0.41176516 || it_count: 8344 || Val Loss: 0.46182005 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 02:12:33.58
Epoch :: 15 || Loss: 0.41078615 || it_count: 8344 || Val Loss: 0.46384962 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 02:21:59.99
Epoch :: 16 || Loss: 0.41095810 || it_count: 8344 || Val Loss: 0.46087168 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 02:31:27.82
Epoch :: 17 || Loss: 0.40947370 || it_count: 8344 || Val Loss: 0.46189921 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 02:40:55.73
Epoch :: 18 || Loss: 0.40940682 || it_count: 8344 || Val Loss: 0.46373267 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 02:50:25.16
Epoch :: 19 || Loss: 0.40752342 || it_count: 8344 || Val Loss: 0.46243097 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 02:59:59.71
Epoch :: 20 || Loss: 0.40754119 || it_count: 8344 || Val Loss: 0.46238169 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 03:09:29.44
Epoch :: 21 || Loss: 0.40817790 || it_count: 8344 || Val Loss: 0.45844233 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 03:18:59.83
Epoch :: 22 || Loss: 0.40717647 || it_count: 8344 || Val Loss: 0.45955528 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 03:28:32.99
Epoch :: 23 || Loss: 0.40659484 || it_count: 8344 || Val Loss: 0.45929194 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 03:38:8.15
Epoch :: 24 || Loss: 0.40793060 || it_count: 8344 || Val Loss: 0.45922899 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 03:47:41.39
Epoch :: 25 || Loss: 0.40716286 || it_count: 8344 || Val Loss: 0.45676246 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 03:57:17.45
Epoch :: 26 || Loss: 0.40613342 || it_count: 8344 || Val Loss: 0.45973535 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 04:07:0.22
Epoch :: 27 || Loss: 0.40566118 || it_count: 8344 || Val Loss: 0.45935294 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 04:16:42.34
Epoch :: 28 || Loss: 0.40544911 || it_count: 8344 || Val Loss: 0.45734178 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 04:26:19.26
Epoch :: 29 || Loss: 0.40465192 || it_count: 8344 || Val Loss: 0.45922059 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 04:35:55.52
Epoch :: 30 || Loss: 0.40999691 || it_count: 8344 || Val Loss: 0.45913578 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 04:45:28.26
Epoch 00015: reducing learning rate of group 0 to 1.0000e-04.
Epoch :: 31 || Loss: 0.40598597 || it_count: 8344 || Val Loss: 0.45890151 || Val it_count: 1107 || Current Learning Rate: 0.0001 || Time: 04:55:1.44
Epoch :: 32 || Loss: 0.41145236 || it_count: 8344 || Val Loss: 0.45506213 || Val it_count: 1107 || Current Learning Rate: 0.0001 || Time: 05:04:36.50
Epoch :: 33 || Loss: 0.40494552 || it_count: 8344 || Val Loss: 0.45622846 || Val it_count: 1107 || Current Learning Rate: 0.0001 || Time: 05:14:10.44
Epoch :: 34 || Loss: 0.40348456 || it_count: 8344 || Val Loss: 0.45556630 || Val it_count: 1107 || Current Learning Rate: 0.0001 || Time: 05:23:42.55
Epoch :: 35 || Loss: 0.40295549 || it_count: 8344 || Val Loss: 0.45572103 || Val it_count: 1107 || Current Learning Rate: 0.0001 || Time: 05:33:15.94
Epoch :: 36 || Loss: 0.40246711 || it_count: 8344 || Val Loss: 0.45613825 || Val it_count: 1107 || Current Learning Rate: 0.0001 || Time: 05:42:51.50
Epoch :: 37 || Loss: 0.40207622 || it_count: 8344 || Val Loss: 0.45651717 || Val it_count: 1107 || Current Learning Rate: 0.0001 || Time: 05:52:24.73
Epoch 00022: reducing learning rate of group 0 to 1.0000e-05.
Epoch :: 38 || Loss: 0.40171346 || it_count: 8344 || Val Loss: 0.45634160 || Val it_count: 1107 || Current Learning Rate: 1e-05 || Time: 06:02:2.40
Epoch :: 39 || Loss: 0.40362948 || it_count: 8344 || Val Loss: 0.44661485 || Val it_count: 1107 || Current Learning Rate: 1e-05 || Time: 06:11:50.46
Epoch :: 40 || Loss: 0.40270308 || it_count: 8344 || Val Loss: 0.44737669 || Val it_count: 1107 || Current Learning Rate: 1e-05 || Time: 06:21:30.22
Epoch :: 41 || Loss: 0.40251696 || it_count: 8344 || Val Loss: 0.44774669 || Val it_count: 1107 || Current Learning Rate: 1e-05 || Time: 06:31:9.03
Epoch :: 42 || Loss: 0.40239437 || it_count: 8344 || Val Loss: 0.44804376 || Val it_count: 1107 || Current Learning Rate: 1e-05 || Time: 06:40:47.21
Epoch :: 43 || Loss: 0.40229717 || it_count: 8344 || Val Loss: 0.44819509 || Val it_count: 1107 || Current Learning Rate: 1e-05 || Time: 06:50:30.88
Epoch :: 44 || Loss: 0.40221582 || it_count: 8344 || Val Loss: 0.44837705 || Val it_count: 1107 || Current Learning Rate: 1e-05 || Time: 07:00:17.93
Epoch 00029: reducing learning rate of group 0 to 1.0000e-06.
Epoch :: 45 || Loss: 0.40213797 || it_count: 8344 || Val Loss: 0.44854528 || Val it_count: 1107 || Current Learning Rate: 1.0000000000000002e-06 || Time: 07:09:56.90
Epoch :: 46 || Loss: 0.40232232 || it_count: 8344 || Val Loss: 0.44681056 || Val it_count: 1107 || Current Learning Rate: 1.0000000000000002e-06 || Time: 07:19:38.10
Epoch :: 47 || Loss: 0.40218597 || it_count: 8344 || Val Loss: 0.44638549 || Val it_count: 1107 || Current Learning Rate: 1.0000000000000002e-06 || Time: 07:29:24.06
Epoch :: 48 || Loss: 0.40213712 || it_count: 8344 || Val Loss: 0.44617580 || Val it_count: 1107 || Current Learning Rate: 1.0000000000000002e-06 || Time: 07:39:4.09
Epoch :: 49 || Loss: 0.40210994 || it_count: 8344 || Val Loss: 0.44606340 || Val it_count: 1107 || Current Learning Rate: 1.0000000000000002e-06 || Time: 07:48:44.81
Epoch :: 50 || Loss: 0.40209137 || it_count: 8344 || Val Loss: 0.44598522 || Val it_count: 1107 || Current Learning Rate: 1.0000000000000002e-06 || Time: 07:58:24.33
Epoch :: 51 || Loss: 0.40207712 || it_count: 8344 || Val Loss: 0.44595202 || Val it_count: 1107 || Current Learning Rate: 1.0000000000000002e-06 || Time: 08:07:57.34
Epoch :: 52 || Loss: 0.40206470 || it_count: 8344 || Val Loss: 0.44593822 || Val it_count: 1107 || Current Learning Rate: 1.0000000000000002e-06 || Time: 08:17:33.22
Epoch :: 53 || Loss: 0.40205415 || it_count: 8344 || Val Loss: 0.44594139 || Val it_count: 1107 || Current Learning Rate: 1.0000000000000002e-06 || Time: 08:27:9.36
Epoch :: 54 || Loss: 0.40204420 || it_count: 8344 || Val Loss: 0.44594837 || Val it_count: 1107 || Current Learning Rate: 1.0000000000000002e-06 || Time: 08:36:47.05
Epoch :: 55 || Loss: 0.40203536 || it_count: 8344 || Val Loss: 0.44596245 || Val it_count: 1107 || Current Learning Rate: 1.0000000000000002e-06 || Time: 08:46:22.51
Epoch :: 56 || Loss: 0.40202671 || it_count: 8344 || Val Loss: 0.44597804 || Val it_count: 1107 || Current Learning Rate: 1.0000000000000002e-06 || Time: 08:55:57.54
Epoch :: 57 || Loss: 0.40201844 || it_count: 8344 || Val Loss: 0.44599732 || Val it_count: 1107 || Current Learning Rate: 1.0000000000000002e-06 || Time: 09:05:29.66
Epoch 00042: reducing learning rate of group 0 to 1.0000e-07.
Early stopping triggered due to learning rate below threshold.
Done Total time: 09:15:6.93
best_loss: 0.44593821854997534

--------------------Testing--------------------

total_samples :: test_dataset = 139200, test_dataloader = 139200
Epoch ::  1 || Loss: 0.38370543 || it_count: 544 || Time: 00:00:23.83
MAE:  0.28863794
MSE:  0.38379416
RMSE:  0.49286175
