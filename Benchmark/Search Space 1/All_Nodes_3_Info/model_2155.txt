--------------------Training--------------------
arch_str :: |lstm_2~0|+|skip_connect~0|lstm_3~1|[relu->dropout->linear->dropout->linear]
model :: 3R
InferCell(
  info :: nodes=3, inC=1, outC=64, [1<-(I0-L0) | 2<-(I0-L1,I1-L2)], genotype_str: lstm_2~0|skip_connect~0|lstm_3~1
  linear_layers: [relu->dropout->linear->dropout->linear]
  (layers): ModuleList(
    (0): LSTM(
      (lstm): LSTM(1, 64, num_layers=2, batch_first=True)
    )
    (1): FactorizedReduce(
      C_in=1, C_out=64, stride=1
      (relu): ReLU()
      (conv): Conv1d(1, 64, kernel_size=(1,), stride=(1,), bias=False)
      (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): LSTM(
      (lstm): LSTM(64, 64, num_layers=3, batch_first=True)
    )
  )
  (linear_layers): ModuleList(
    (0): ReLU()
    (1): Dropout(p=0.1, inplace=False)
    (2): Linear(in_features=3072, out_features=1536, bias=True)
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=1536, out_features=1, bias=True)
  )
)
tr_params :: epochs = 100, patience = 20, batch_size = 256, window_size = 48, data_size = 100
Model FLOPs: 12.071M, Model Params: 4.872M
learning rate scheduling :: (optimizer, mode='min', factor=0.1, patience=5, verbose=True)
total_samples :: train_dataset = 2136000, train_dataloader = 2136000
total_samples :: valid_dataset = 283200, valid_dataloader = 283200
Epoch ::  1 || Loss: 0.48302461 || it_count: 8344 || Val Loss: 0.50270384 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 00:04:31.10
Epoch ::  2 || Loss: 0.49414936 || it_count: 8344 || Val Loss: 0.51327850 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 00:09:0.02
Epoch ::  3 || Loss: 0.48582878 || it_count: 8344 || Val Loss: 0.49469612 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 00:13:31.01
Epoch ::  4 || Loss: 0.47546598 || it_count: 8344 || Val Loss: 0.52183608 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 00:18:3.82
Epoch ::  5 || Loss: 0.46077087 || it_count: 8344 || Val Loss: 0.54012992 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 00:22:36.04
Epoch ::  6 || Loss: 0.46128097 || it_count: 8344 || Val Loss: 0.50749210 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 00:27:7.99
Epoch ::  7 || Loss: 0.45519727 || it_count: 8344 || Val Loss: 0.49829243 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 00:31:39.95
Epoch ::  8 || Loss: 0.44960463 || it_count: 8344 || Val Loss: 0.50035821 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 00:36:11.60
Epoch ::  9 || Loss: 0.44559761 || it_count: 8344 || Val Loss: 0.48980954 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 00:40:43.47
Epoch :: 10 || Loss: 0.44167560 || it_count: 8344 || Val Loss: 0.48266811 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 00:45:14.50
Epoch :: 11 || Loss: 0.43959571 || it_count: 8344 || Val Loss: 0.47850571 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 00:49:46.82
Epoch :: 12 || Loss: 0.43755872 || it_count: 8344 || Val Loss: 0.48532560 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 00:54:17.66
Epoch :: 13 || Loss: 0.43677532 || it_count: 8344 || Val Loss: 0.47722494 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 00:58:50.33
Epoch :: 14 || Loss: 0.43543892 || it_count: 8344 || Val Loss: 0.48055497 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 01:03:22.02
Epoch :: 15 || Loss: 0.43569657 || it_count: 8344 || Val Loss: 0.48009615 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 01:07:51.37
Epoch :: 16 || Loss: 0.43482069 || it_count: 8344 || Val Loss: 0.48403413 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 01:12:23.90
Epoch :: 17 || Loss: 0.43472388 || it_count: 8344 || Val Loss: 0.47741822 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 01:16:56.52
Epoch :: 18 || Loss: 0.43406567 || it_count: 8344 || Val Loss: 0.47795733 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 01:21:28.05
Epoch :: 19 || Loss: 0.43415972 || it_count: 8344 || Val Loss: 0.47798154 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 01:25:58.42
Epoch :: 20 || Loss: 0.43398956 || it_count: 8344 || Val Loss: 0.48381562 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 01:30:30.65
Epoch :: 21 || Loss: 0.43522103 || it_count: 8344 || Val Loss: 0.47706834 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 01:35:2.02
Epoch :: 22 || Loss: 0.43915084 || it_count: 8344 || Val Loss: 0.48889726 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 01:39:39.10
Epoch :: 23 || Loss: 0.43346103 || it_count: 8344 || Val Loss: 0.48520636 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 01:44:13.48
Epoch :: 24 || Loss: 0.43386917 || it_count: 8344 || Val Loss: 0.47724830 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 01:48:45.35
Epoch :: 25 || Loss: 0.43248961 || it_count: 8344 || Val Loss: 0.48375397 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 01:53:19.24
Epoch :: 26 || Loss: 0.43006241 || it_count: 8344 || Val Loss: 0.47225026 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 01:57:52.59
Epoch :: 27 || Loss: 0.42315253 || it_count: 8344 || Val Loss: 0.46495075 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 02:02:23.59
Epoch :: 28 || Loss: 0.41876225 || it_count: 8344 || Val Loss: 0.46937142 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 02:06:54.38
Epoch :: 29 || Loss: 0.41760291 || it_count: 8344 || Val Loss: 0.46661204 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 02:11:27.88
Epoch :: 30 || Loss: 0.41750058 || it_count: 8344 || Val Loss: 0.46879411 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 02:16:0.69
Epoch :: 31 || Loss: 0.41633933 || it_count: 8344 || Val Loss: 0.46999149 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 02:20:36.28
Epoch :: 32 || Loss: 0.41654300 || it_count: 8344 || Val Loss: 0.47049029 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 02:25:8.84
Epoch :: 33 || Loss: 0.41542304 || it_count: 8344 || Val Loss: 0.47364125 || Val it_count: 1107 || Current Learning Rate: 0.0001 || Time: 02:29:41.97
Epoch :: 34 || Loss: 0.41867641 || it_count: 8344 || Val Loss: 0.46180394 || Val it_count: 1107 || Current Learning Rate: 0.0001 || Time: 02:34:15.14
Epoch :: 35 || Loss: 0.41391839 || it_count: 8344 || Val Loss: 0.45980314 || Val it_count: 1107 || Current Learning Rate: 0.0001 || Time: 02:38:47.96
Epoch :: 36 || Loss: 0.41281102 || it_count: 8344 || Val Loss: 0.45927829 || Val it_count: 1107 || Current Learning Rate: 0.0001 || Time: 02:43:22.91
Epoch :: 37 || Loss: 0.41215154 || it_count: 8344 || Val Loss: 0.45938809 || Val it_count: 1107 || Current Learning Rate: 0.0001 || Time: 02:47:57.42
Epoch :: 38 || Loss: 0.41161294 || it_count: 8344 || Val Loss: 0.45793912 || Val it_count: 1107 || Current Learning Rate: 0.0001 || Time: 02:52:32.27
Epoch :: 39 || Loss: 0.41108678 || it_count: 8344 || Val Loss: 0.45954133 || Val it_count: 1107 || Current Learning Rate: 0.0001 || Time: 02:57:6.47
Epoch :: 40 || Loss: 0.41070657 || it_count: 8344 || Val Loss: 0.45849603 || Val it_count: 1107 || Current Learning Rate: 0.0001 || Time: 03:01:37.75
Epoch :: 41 || Loss: 0.40936246 || it_count: 8344 || Val Loss: 0.45822337 || Val it_count: 1107 || Current Learning Rate: 0.0001 || Time: 03:06:10.22
Epoch :: 42 || Loss: 0.40829137 || it_count: 8344 || Val Loss: 0.45702203 || Val it_count: 1107 || Current Learning Rate: 0.0001 || Time: 03:10:44.18
Epoch :: 43 || Loss: 0.40784718 || it_count: 8344 || Val Loss: 0.45666476 || Val it_count: 1107 || Current Learning Rate: 0.0001 || Time: 03:15:18.50
Epoch :: 44 || Loss: 0.40754324 || it_count: 8344 || Val Loss: 0.45502405 || Val it_count: 1107 || Current Learning Rate: 0.0001 || Time: 03:19:49.35
Epoch :: 45 || Loss: 0.40859774 || it_count: 8344 || Val Loss: 0.45983070 || Val it_count: 1107 || Current Learning Rate: 0.0001 || Time: 03:24:23.08
Epoch :: 46 || Loss: 0.40697555 || it_count: 8344 || Val Loss: 0.45775687 || Val it_count: 1107 || Current Learning Rate: 0.0001 || Time: 03:28:56.43
Epoch :: 47 || Loss: 0.40666759 || it_count: 8344 || Val Loss: 0.45245152 || Val it_count: 1107 || Current Learning Rate: 0.0001 || Time: 03:33:29.71
Epoch :: 48 || Loss: 0.40608316 || it_count: 8344 || Val Loss: 0.45382484 || Val it_count: 1107 || Current Learning Rate: 0.0001 || Time: 03:38:3.22
Epoch :: 49 || Loss: 0.40532702 || it_count: 8344 || Val Loss: 0.45458585 || Val it_count: 1107 || Current Learning Rate: 0.0001 || Time: 03:43:9.96
Epoch :: 50 || Loss: 0.40487102 || it_count: 8344 || Val Loss: 0.44998299 || Val it_count: 1107 || Current Learning Rate: 0.0001 || Time: 03:47:47.27
Epoch :: 51 || Loss: 0.40443324 || it_count: 8344 || Val Loss: 0.45592117 || Val it_count: 1107 || Current Learning Rate: 0.0001 || Time: 03:52:36.06
Epoch :: 52 || Loss: 0.40476899 || it_count: 8344 || Val Loss: 0.45469694 || Val it_count: 1107 || Current Learning Rate: 0.0001 || Time: 03:57:30.12
Epoch :: 53 || Loss: 0.40404916 || it_count: 8344 || Val Loss: 0.45491083 || Val it_count: 1107 || Current Learning Rate: 0.0001 || Time: 04:02:1.44
Epoch :: 54 || Loss: 0.40347524 || it_count: 8344 || Val Loss: 0.45667460 || Val it_count: 1107 || Current Learning Rate: 0.0001 || Time: 04:06:31.82
Epoch :: 55 || Loss: 0.40343968 || it_count: 8344 || Val Loss: 0.45480309 || Val it_count: 1107 || Current Learning Rate: 0.0001 || Time: 04:11:5.17
Epoch :: 56 || Loss: 0.40312490 || it_count: 8344 || Val Loss: 0.45640240 || Val it_count: 1107 || Current Learning Rate: 1e-05 || Time: 04:15:38.96
Epoch :: 57 || Loss: 0.40584609 || it_count: 8344 || Val Loss: 0.44335713 || Val it_count: 1107 || Current Learning Rate: 1e-05 || Time: 04:20:10.57
Epoch :: 58 || Loss: 0.40431910 || it_count: 8344 || Val Loss: 0.44316372 || Val it_count: 1107 || Current Learning Rate: 1e-05 || Time: 04:24:43.18
Epoch :: 59 || Loss: 0.40409556 || it_count: 8344 || Val Loss: 0.44365176 || Val it_count: 1107 || Current Learning Rate: 1e-05 || Time: 04:29:16.37
Epoch :: 60 || Loss: 0.40379848 || it_count: 8344 || Val Loss: 0.44339848 || Val it_count: 1107 || Current Learning Rate: 1e-05 || Time: 04:33:49.24
Epoch :: 61 || Loss: 0.40380904 || it_count: 8344 || Val Loss: 0.44345583 || Val it_count: 1107 || Current Learning Rate: 1e-05 || Time: 04:38:24.48
Epoch :: 62 || Loss: 0.40381519 || it_count: 8344 || Val Loss: 0.44339489 || Val it_count: 1107 || Current Learning Rate: 1e-05 || Time: 04:42:57.91
Epoch :: 63 || Loss: 0.40363180 || it_count: 8344 || Val Loss: 0.44350375 || Val it_count: 1107 || Current Learning Rate: 1e-05 || Time: 04:47:32.94
Epoch :: 64 || Loss: 0.40343663 || it_count: 8344 || Val Loss: 0.44426898 || Val it_count: 1107 || Current Learning Rate: 1.0000000000000002e-06 || Time: 04:52:6.87
Epoch :: 65 || Loss: 0.40400440 || it_count: 8344 || Val Loss: 0.43810656 || Val it_count: 1107 || Current Learning Rate: 1.0000000000000002e-06 || Time: 04:56:42.16
Epoch :: 66 || Loss: 0.40383846 || it_count: 8344 || Val Loss: 0.43745406 || Val it_count: 1107 || Current Learning Rate: 1.0000000000000002e-06 || Time: 05:01:17.68
Epoch :: 67 || Loss: 0.40367973 || it_count: 8344 || Val Loss: 0.43727020 || Val it_count: 1107 || Current Learning Rate: 1.0000000000000002e-06 || Time: 05:05:51.38
Epoch :: 68 || Loss: 0.40351371 || it_count: 8344 || Val Loss: 0.43730626 || Val it_count: 1107 || Current Learning Rate: 1.0000000000000002e-06 || Time: 05:10:25.58
Epoch :: 69 || Loss: 0.40362074 || it_count: 8344 || Val Loss: 0.43728403 || Val it_count: 1107 || Current Learning Rate: 1.0000000000000002e-06 || Time: 05:14:58.89
Epoch :: 70 || Loss: 0.40358552 || it_count: 8344 || Val Loss: 0.43726602 || Val it_count: 1107 || Current Learning Rate: 1.0000000000000002e-06 || Time: 05:19:33.34
Epoch :: 71 || Loss: 0.40345628 || it_count: 8344 || Val Loss: 0.43724166 || Val it_count: 1107 || Current Learning Rate: 1.0000000000000002e-06 || Time: 05:24:6.71
Epoch :: 72 || Loss: 0.40357220 || it_count: 8344 || Val Loss: 0.43724089 || Val it_count: 1107 || Current Learning Rate: 1.0000000000000002e-06 || Time: 05:28:42.38
Early stopping triggered due to learning rate below threshold.
Done Total time: 05:33:16.47
best_loss: 0.4372408914553113

--------------------Testing--------------------

total_samples :: test_dataset = 139200, test_dataloader = 139200
Epoch ::  1 || Loss: 0.29472467 || it_count: 544 || Time: 00:00:13.94
MAE:  0.28363267
MSE:  0.29477054
RMSE:  0.47968015
