--------------------Training--------------------
arch_str :: |lstm_1~0|+|skip_connect~0|skip_connect~1|[dropout->linear->dropout->linear]
model :: 3J
InferCell(
  info :: nodes=3, inC=1, outC=64, [1<-(I0-L0) | 2<-(I0-L1,I1-L2)], genotype_str: lstm_1~0|skip_connect~0|skip_connect~1
  linear_layers: [dropout->linear->dropout->linear]
  (layers): ModuleList(
    (0): LSTM(
      (lstm): LSTM(1, 64, batch_first=True)
    )
    (1): FactorizedReduce(
      C_in=1, C_out=64, stride=1
      (relu): ReLU()
      (conv): Conv1d(1, 64, kernel_size=(1,), stride=(1,), bias=False)
      (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): Identity()
  )
  (linear_layers): ModuleList(
    (0): Dropout(p=0.1, inplace=False)
    (1): Linear(in_features=3072, out_features=1536, bias=True)
    (2): Dropout(p=0.1, inplace=False)
    (3): Linear(in_features=1536, out_features=1, bias=True)
  )
)
tr_params :: epochs = 100, patience = 20, batch_size = 256, window_size = 48, data_size = 100
Model MACs: 5.583M, Model Params: 4.739M
learning rate scheduling :: (optimizer, mode='min', factor=0.1, patience=5, verbose=True)
total_samples :: train_dataset = 2136000, train_dataloader = 2136000
total_samples :: valid_dataset = 283200, valid_dataloader = 283200
Epoch ::  1 || Loss: 0.44819315 || it_count: 8344 || Val Loss: 0.48070783 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 00:03:32.95
Epoch ::  2 || Loss: 0.41644866 || it_count: 8344 || Val Loss: 0.46836233 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 00:06:59.30
Epoch ::  3 || Loss: 0.41459112 || it_count: 8344 || Val Loss: 0.46632622 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 00:10:25.08
Epoch ::  4 || Loss: 0.41356897 || it_count: 8344 || Val Loss: 0.47374438 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 00:13:50.68
Epoch ::  5 || Loss: 0.41273730 || it_count: 8344 || Val Loss: 0.46825891 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 00:17:14.50
Epoch ::  6 || Loss: 0.41240198 || it_count: 8344 || Val Loss: 0.46917876 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 00:20:39.16
Epoch ::  7 || Loss: 0.41220211 || it_count: 8344 || Val Loss: 0.46986066 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 00:24:4.73
Epoch ::  8 || Loss: 0.41233131 || it_count: 8344 || Val Loss: 0.46782756 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 00:27:29.08
Epoch ::  9 || Loss: 0.41246662 || it_count: 8344 || Val Loss: 0.46933718 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 00:30:53.53
Epoch :: 10 || Loss: 0.41194898 || it_count: 8344 || Val Loss: 0.47012569 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 00:34:19.30
Epoch :: 11 || Loss: 0.41223564 || it_count: 8344 || Val Loss: 0.46967109 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 00:37:46.27
Epoch :: 12 || Loss: 0.41177287 || it_count: 8344 || Val Loss: 0.47239911 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 00:41:12.66
Epoch :: 13 || Loss: 0.41198003 || it_count: 8344 || Val Loss: 0.46980753 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 00:44:39.16
Epoch :: 14 || Loss: 0.41156860 || it_count: 8344 || Val Loss: 0.46997502 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 00:48:7.77
Epoch :: 15 || Loss: 0.41212772 || it_count: 8344 || Val Loss: 0.47174475 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 00:51:35.62
Epoch :: 16 || Loss: 0.41169010 || it_count: 8344 || Val Loss: 0.47338768 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 00:55:4.29
Epoch :: 17 || Loss: 0.41162521 || it_count: 8344 || Val Loss: 0.47583767 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 00:58:31.43
Epoch :: 18 || Loss: 0.41144864 || it_count: 8344 || Val Loss: 0.47289542 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 01:01:58.01
Epoch :: 19 || Loss: 0.41170318 || it_count: 8344 || Val Loss: 0.47140474 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 01:05:25.81
Epoch :: 20 || Loss: 0.41196700 || it_count: 8344 || Val Loss: 0.46714838 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 01:08:53.46
Epoch :: 21 || Loss: 0.41144415 || it_count: 8344 || Val Loss: 0.46950358 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 01:12:22.78
Epoch :: 22 || Loss: 0.41117499 || it_count: 8344 || Val Loss: 0.47044184 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 01:15:48.27
Epoch :: 23 || Loss: 0.41150608 || it_count: 8344 || Val Loss: 0.47268906 || Val it_count: 1107 || Current Learning Rate: 0.001 || Time: 01:19:16.41
Early stopping triggered due to patience exceeded.
Done Total time: 01:19:16.41
best_loss: 0.46632622050335915

--------------------Testing--------------------

total_samples :: test_dataset = 139200, test_dataloader = 139200
Epoch ::  1 || Loss: 0.39078909 || it_count: 544 || Time: 00:00:11.30
MAE:  0.31755114
MSE:  0.39088747
RMSE:  0.5161464
